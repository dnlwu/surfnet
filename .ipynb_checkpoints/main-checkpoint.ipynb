{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader as web\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pywt\n",
    "import seaborn\n",
    "from statsmodels.robust import mad\n",
    "from scipy import signal\n",
    "import data_reader, features\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras import optimizers\n",
    "# import numpy as np\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(df):\n",
    "    df['returns'] = df.pct_change()\n",
    "    df['log-returns'] = np.log(df.iloc[:,0]).diff()\n",
    "    df['up-down'] = np.sign(df['log-returns'])\n",
    "    df_dropna = df.dropna()\n",
    "    return df, df_dropna\n",
    "\n",
    "def remove_na(df):\n",
    "    df = df[df['returns'].notna()]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cwt_features(scale_bot,scale_top,scale_incr,data):\n",
    "    scales = np.arange(scale_bot,scale_top,step=scale_incr)\n",
    "\n",
    "    cwt = features.plot_wavelet(time, data, scales)\n",
    "    # print(type(cwt))\n",
    "    cwt_features = pd.DataFrame(cwt).T\n",
    "    cwt_features.set_index(returns.index,inplace=True)\n",
    "    return cwt_features\n",
    "\n",
    "def prep_features(data,history_points):\n",
    "    hist = np.array([data[i:i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    return hist\n",
    "\n",
    "def prep_labels(data,history_points):\n",
    "    hist_labels = np.array([data[i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    hist_labels = np.expand_dims(hist_labels, -1)\n",
    "    return hist_labels\n",
    "\n",
    "def split_data(feats, labels, test_split):\n",
    "    assert feats.shape[0] == labels.shape[0]\n",
    "    n = int(labels.shape[0]*test_split)\n",
    "    feature_train = feats[:n]\n",
    "    label_train = labels[:n]\n",
    "    feature_test = feats[n:]\n",
    "    label_test = labels[n:]\n",
    "    return feature_train, label_train, feature_test, label_test\n",
    "\n",
    "def test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "#     output = Activation('sigmoid', name='linear_output')(x)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "# not used\n",
    "def test2(hist_feats,feature_train,feature_test,label_train,label_test,epoch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "\n",
    "    y = LSTM(50, name='lstm_1')(x)\n",
    "    y = Dropout(0.2, name='lstm_dropout_1')(y)\n",
    "    y = Dense(64, name='dense_0')(y)\n",
    "    y = Activation('sigmoid', name='sigmoid_0')(y)\n",
    "    y = Dense(1, name='dense_1')(y)\n",
    "\n",
    "    output = Activation('sigmoid', name='linear_output')(y)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "def test_stats(predicted, real):\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if (predicted[i] > 0) and (real[i] > 0):\n",
    "            c = c+1\n",
    "        if (predicted[i] < 0) and (real[i] < 0):\n",
    "            c = c+1\n",
    "        s = s+1\n",
    "    print('da',c/s)\n",
    "    pct_correct_da = c/s\n",
    "    \n",
    "    return pct_correct_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adjusted close   returns  log-returns  up-down\n",
      "2018-07-02        182.1920       NaN          NaN      NaN\n",
      "2018-07-03        179.0188 -0.017417    -0.017570     -1.0\n",
      "2018-07-05        180.4594  0.008047     0.008015      1.0\n",
      "2018-07-06        182.9609  0.013862     0.013767      1.0\n",
      "2018-07-09        185.5014  0.013885     0.013790      1.0\n",
      "...                    ...       ...          ...      ...\n",
      "2018-12-24        143.9221 -0.025874    -0.026215     -1.0\n",
      "2018-12-26        154.0573  0.070421     0.068052      1.0\n",
      "2018-12-27        153.0575 -0.006490    -0.006511     -1.0\n",
      "2018-12-28        153.1360  0.000513     0.000513      1.0\n",
      "2018-12-31        154.6161  0.009665     0.009619      1.0\n",
      "\n",
      "[126 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "start = '2018-07-01'\n",
    "end = '2019-01-01'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "df = data_reader.download(ticker,start,end)\n",
    "\n",
    "\n",
    "opens = df['adjusted close'].to_frame()\n",
    "opens, returns = calc_returns(opens)\n",
    "print(opens)\n",
    "\n",
    "\n",
    "signal = df['adjusted close'].dropna().to_numpy()\n",
    "log_signal = returns['log-returns'].dropna().to_numpy()\n",
    "\n",
    "\n",
    "data = log_signal\n",
    "N = len(data)\n",
    "t0=0\n",
    "dt=1/365\n",
    "time = np.arange(0, N) * dt + t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = returns['log-returns'].dropna().to_numpy() \n",
    "scale_bot = 1\n",
    "scale_top = 10\n",
    "scale_incr = 1\n",
    "cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num lbls: 125\n",
      "num results: 126\n"
     ]
    }
   ],
   "source": [
    "results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "print('num lbls:',len(lbls))\n",
    "print('num results:',len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adjusted close   returns  log-returns  up-down\n",
      "2002-01-02          1.4407       NaN          NaN      NaN\n",
      "2002-01-03          1.4580  0.012008     0.011937      1.0\n",
      "2002-01-04          1.4648  0.004664     0.004653      1.0\n",
      "2002-01-07          1.4159 -0.033383    -0.033953     -1.0\n",
      "2002-01-08          1.3980 -0.012642    -0.012723     -1.0\n",
      "...                    ...       ...          ...      ...\n",
      "2019-01-04        145.3238  0.042689     0.041803      1.0\n",
      "2019-01-07        145.0003 -0.002226    -0.002229     -1.0\n",
      "2019-01-08        147.7645  0.019063     0.018884      1.0\n",
      "2019-01-09        150.2738  0.016982     0.016839      1.0\n",
      "2019-01-10        150.7541  0.003196     0.003191      1.0\n",
      "\n",
      "[4286 rows x 4 columns]\n",
      "running: hist 10, cwt top 10,batch 8\n",
      "WARNING:tensorflow:From c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py:853: RuntimeWarning: invalid value encountered in sign\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3462 samples, validate on 385 samples\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(8, 11), b.shape=(11, 200), m=8, n=200, k=11\n\t [[{{node lstm_0/while/MatMul}}]]\n\t [[Mean/_93]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(8, 11), b.shape=(11, 200), m=8, n=200, k=11\n\t [[{{node lstm_0/while/MatMul}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d2c27a51e891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_feats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[0mpct_da\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5e144e2c3d81>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(hist_feats, feature_train, feature_test, label_train, label_test, epoch, batch)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(8, 11), b.shape=(11, 200), m=8, n=200, k=11\n\t [[{{node lstm_0/while/MatMul}}]]\n\t [[Mean/_93]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(8, 11), b.shape=(11, 200), m=8, n=200, k=11\n\t [[{{node lstm_0/while/MatMul}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "start = '2002-01-01'\n",
    "end = '2019-01-10'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "df = data_reader.download(ticker,start,end)\n",
    "\n",
    "\n",
    "opens = df['adjusted close'].to_frame()\n",
    "opens, returns = calc_returns(opens)\n",
    "print(opens)\n",
    "\n",
    "\n",
    "signal = df['adjusted close'].dropna().to_numpy()\n",
    "log_signal = returns['log-returns'].dropna().to_numpy()\n",
    "\n",
    "\n",
    "data = log_signal\n",
    "N = len(data)\n",
    "t0=0\n",
    "dt=1/365\n",
    "time = np.arange(0, N) * dt + t0\n",
    "\n",
    "\n",
    "# scale_bot = 1 ########## UNCOMMENT ##########\n",
    "# scale_top = 80 ########## UNCOMMENT ##########\n",
    "# scale_incr = 1 ########## UNCOMMENT ##########\n",
    "\n",
    "# cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data) ########## UNCOMMENT ##########\n",
    "# results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False) ########## UNCOMMENT ##########\n",
    "# results = pd.concat([opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "# print(results)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# feats = cwt_features.to_numpy()\n",
    "# feats = results.dropna().to_numpy() ########## UNCOMMENT ##########\n",
    "###################################################################\n",
    "\n",
    "# history_points = 100\n",
    "\n",
    "# hist_cwt = prep_features(cwt_features_np,history_points) ####### either this or next line. try next\n",
    "# hist_feats = prep_features(feats,history_points) ########## UNCOMMENT ##########\n",
    "\n",
    "\n",
    "lbls = returns['log-returns'].dropna().to_numpy() \n",
    "# hist_labels = prep_labels(lbls,history_points) ########## UNCOMMENT ##########\n",
    "# print(results['log-returns'])\n",
    "# print(hist_feats)\n",
    "# print(hist_labels)\n",
    "\n",
    "# print(hist_labels.shape)\n",
    "# print(hist_feats.shape)\n",
    "\n",
    "test_split = 0.9\n",
    "\n",
    "# feature_train, label_train, feature_test, label_test = split_data(hist_feats,hist_labels,0.9) ########## UNCOMMENT ##########\n",
    "\n",
    "# print('feat train',feature_train.shape)\n",
    "# print('label train',label_train.shape)\n",
    "# print('feat test',feature_test.shape)\n",
    "# print('label test',label_test.shape)\n",
    "\n",
    "\n",
    "# epoch = 100 ########## UNCOMMENT ##########\n",
    "# batch = 32 ########## UNCOMMENT ##########\n",
    "# predicted, real = test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch) ########## UNCOMMENT ##########\n",
    "# pct_da = test_stats(predicted, real) ########## UNCOMMENT ##########\n",
    "# print('epoch:',epoch) ########## UNCOMMENT ##########\n",
    "# print('startdate:',start) ########## UNCOMMENT ##########\n",
    "# print('enddate:',end) ########## UNCOMMENT ##########\n",
    "# print('cwt splits:',scale_bot,scale_top,scale_incr) ########## UNCOMMENT ##########\n",
    "# print('hist dates',history_points) ########## UNCOMMENT ##########\n",
    "\n",
    "report = pd.DataFrame(columns=['Epoch','Batch Size','CWT Top','CWT Incr','Hist points','Pct Acc'])\n",
    "\n",
    "\n",
    "batches = [8]\n",
    "tops = [10]\n",
    "hist_list = [10]\n",
    "epochs = [150]\n",
    "for i in tops:\n",
    "    for b in batches:\n",
    "        for k in hist_list:\n",
    "            for e in epochs:\n",
    "                \n",
    "                print(\"running: hist \" + str(k) + \", cwt top \"+ str(i) + \",batch \"+str(b) )\n",
    "                scale_bot = 1\n",
    "                scale_top = i\n",
    "                scale_incr = 1\n",
    "\n",
    "                cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data)\n",
    "                results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "\n",
    "                feats = results.dropna().to_numpy()\n",
    "                history_points = k\n",
    "                hist_feats = prep_features(feats,history_points)\n",
    "\n",
    "                # lbls = returns['log-returns'].dropna().to_numpy()\n",
    "                hist_labels = prep_labels(lbls,history_points)\n",
    "\n",
    "                \n",
    "                feature_train, label_train, feature_test, label_test = split_data(hist_feats,hist_labels,0.9)\n",
    "\n",
    "                epoch = e\n",
    "                batch = b \n",
    "                predicted, real = test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch) \n",
    "                pct_da = test_stats(predicted, real) \n",
    "\n",
    "                report = report.append({'Epoch':e,'Batch Size':b,'CWT Top':i,'CWT Incr':1,'Hist points':k,'Pct Acc':pct_da},ignore_index=True)\n",
    "                \n",
    "                test_datapoints = pd.DataFrame(data={'Pred':predicted.T[0],'Actual':real.T[0]})\n",
    "                fname_test_dpoints = str(ticker)+'_datapoints_e'+str(e)+'b'+str(b)+'cwttop'+str(i)+'cwtinc'+str(1)+'h'+str(k)+'pctacc'+str(pct_da)+'.csv'\n",
    "                data_reader.save_df(test_datapoints,fname_test_dpoints)\n",
    "\n",
    "data_reader.save_df(report,'AAPL_hypertests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
