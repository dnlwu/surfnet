{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as web\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pywt\n",
    "import seaborn\n",
    "from statsmodels.robust import mad\n",
    "from scipy import signal\n",
    "import data_reader, features\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras import optimizers\n",
    "# import numpy as np\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_returns(df):\n",
    "    df['returns'] = df.pct_change()\n",
    "    df['log-returns'] = np.log(df.iloc[:,0]).diff()\n",
    "    df['up-down'] = np.sign(df['log-returns'])\n",
    "    df_dropna = df.dropna()\n",
    "    return df, df_dropna\n",
    "\n",
    "def remove_na(df):\n",
    "    df = df[df['returns'].notna()]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cwt_features(scale_bot,scale_top,scale_incr,data):\n",
    "    scales = np.arange(scale_bot,scale_top,step=scale_incr)\n",
    "\n",
    "    cwt = features.plot_wavelet(time, data, scales)\n",
    "    # print(type(cwt))\n",
    "    cwt_features = pd.DataFrame(cwt).T\n",
    "    cwt_features.set_index(returns.index,inplace=True)\n",
    "    return cwt_features\n",
    "\n",
    "def prep_features(data,history_points):\n",
    "    hist = np.array([data[i:i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    return hist\n",
    "\n",
    "def prep_labels(data,history_points):\n",
    "    hist_labels = np.array([data[i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    hist_labels = np.expand_dims(hist_labels, -1)\n",
    "    return hist_labels\n",
    "\n",
    "def split_data(feats, labels, test_split):\n",
    "    assert feats.shape[0] == labels.shape[0]\n",
    "    n = int(labels.shape[0]*test_split)\n",
    "    feature_train = feats[:n]\n",
    "    label_train = labels[:n]\n",
    "    feature_test = feats[n:]\n",
    "    label_test = labels[n:]\n",
    "    return feature_train, label_train, feature_test, label_test\n",
    "\n",
    "def test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "#     output = Activation('sigmoid', name='linear_output')(x)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "# not used\n",
    "def test2(hist_feats,feature_train,feature_test,label_train,label_test,epoch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "\n",
    "    y = LSTM(50, name='lstm_1')(x)\n",
    "    y = Dropout(0.2, name='lstm_dropout_1')(y)\n",
    "    y = Dense(64, name='dense_0')(y)\n",
    "    y = Activation('sigmoid', name='sigmoid_0')(y)\n",
    "    y = Dense(1, name='dense_1')(y)\n",
    "\n",
    "    output = Activation('sigmoid', name='linear_output')(y)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "def test_stats(predicted, real):\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if (predicted[i] > 0) and (real[i] > 0):\n",
    "            c = c+1\n",
    "        if (predicted[i] < 0) and (real[i] < 0):\n",
    "            c = c+1\n",
    "        s = s+1\n",
    "    print('da',c/s)\n",
    "    pct_correct_da = c/s\n",
    "    \n",
    "    return pct_correct_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adjusted close   returns  log-returns  up-down\n",
      "2002-01-02          1.4407       NaN          NaN      NaN\n",
      "2002-01-03          1.4580  0.012008     0.011937      1.0\n",
      "2002-01-04          1.4648  0.004664     0.004653      1.0\n",
      "2002-01-07          1.4159 -0.033383    -0.033953     -1.0\n",
      "2002-01-08          1.3980 -0.012642    -0.012723     -1.0\n",
      "...                    ...       ...          ...      ...\n",
      "2019-01-04        145.3238  0.042689     0.041803      1.0\n",
      "2019-01-07        145.0003 -0.002226    -0.002229     -1.0\n",
      "2019-01-08        147.7645  0.019063     0.018884      1.0\n",
      "2019-01-09        150.2738  0.016982     0.016839      1.0\n",
      "2019-01-10        150.7541  0.003196     0.003191      1.0\n",
      "\n",
      "[4286 rows x 4 columns]\n",
      "running: hist 10, cwt top 10,batch 8\n",
      "Train on 3462 samples, validate on 385 samples\n",
      "Epoch 1/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 0.0581 - val_loss: 4.9520e-04\n",
      "Epoch 2/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 0.0013 - val_loss: 2.1941e-04\n",
      "Epoch 3/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 8.6567e-04 - val_loss: 2.1433e-04\n",
      "Epoch 4/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 7.7326e-04 - val_loss: 2.0651e-04\n",
      "Epoch 5/150\n",
      " 336/3462 [=>............................] - ETA: 4s - loss: 5.9756e-04"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "start = '2002-01-01'\n",
    "end = '2019-01-10'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "df = data_reader.download(ticker,start,end)\n",
    "\n",
    "\n",
    "opens = df['adjusted close'].to_frame()\n",
    "opens, returns = calc_returns(opens)\n",
    "print(opens)\n",
    "\n",
    "# generate log signal\n",
    "signal = df['adjusted close'].dropna().to_numpy()\n",
    "log_signal = returns['log-returns'].dropna().to_numpy()\n",
    "\n",
    "\n",
    "data = log_signal\n",
    "N = len(data)\n",
    "t0=0\n",
    "dt=1/365\n",
    "time = np.arange(0, N) * dt + t0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lbls = returns['log-returns'].dropna().to_numpy() \n",
    "\n",
    "test_split = 0.9\n",
    "\n",
    "\n",
    "# report for saving testing results\n",
    "report = pd.DataFrame(columns=['Epoch','Batch Size','CWT Top','CWT Incr','Hist points','Pct Acc'])\n",
    "\n",
    "# set hyperparameters\n",
    "batches = [8]\n",
    "tops = [10]\n",
    "hist_list = [10]\n",
    "epochs = [150]\n",
    "for i in tops:\n",
    "    for b in batches:\n",
    "        for k in hist_list:\n",
    "            for e in epochs:\n",
    "                \n",
    "                print(\"running: hist \" + str(k) + \", cwt top \"+ str(i) + \",batch \"+str(b) )\n",
    "                scale_bot = 1\n",
    "                scale_top = i\n",
    "                scale_incr = 1\n",
    "\n",
    "                cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data)\n",
    "                results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "\n",
    "                feats = results.dropna().to_numpy()\n",
    "                history_points = k\n",
    "                hist_feats = prep_features(feats,history_points)\n",
    "\n",
    "                hist_labels = prep_labels(lbls,history_points)\n",
    "\n",
    "                \n",
    "                feature_train, label_train, feature_test, label_test = split_data(hist_feats,hist_labels,0.9)\n",
    "\n",
    "                epoch = e\n",
    "                batch = b \n",
    "                predicted, real = test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch) \n",
    "                pct_da = test_stats(predicted, real) \n",
    "\n",
    "                report = report.append({'Epoch':e,'Batch Size':b,'CWT Top':i,'CWT Incr':1,'Hist points':k,'Pct Acc':pct_da},ignore_index=True)\n",
    "                \n",
    "                test_datapoints = pd.DataFrame(data={'Pred':predicted.T[0],'Actual':real.T[0]})\n",
    "                fname_test_dpoints = str(ticker)+'_datapoints_e'+str(e)+'b'+str(b)+'cwttop'+str(i)+'cwtinc'+str(1)+'h'+str(k)+'pctacc'+str(pct_da)+'.csv'\n",
    "                data_reader.save_df(test_datapoints,fname_test_dpoints)\n",
    "\n",
    "data_reader.save_df(report,'AAPL_hypertests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
