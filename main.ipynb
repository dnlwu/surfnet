{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as web\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pywt\n",
    "import seaborn\n",
    "from statsmodels.robust import mad\n",
    "from scipy import signal\n",
    "import data_reader, features\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras import optimizers\n",
    "# import numpy as np\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(df):\n",
    "    df['returns'] = df.pct_change()\n",
    "    df['log-returns'] = np.log(df.iloc[:,0]).diff()\n",
    "    df['up-down'] = np.sign(df['log-returns'])\n",
    "    df_dropna = df.dropna()\n",
    "    return df, df_dropna\n",
    "\n",
    "def remove_na(df):\n",
    "    df = df[df['returns'].notna()]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cwt_features(scale_bot,scale_top,scale_incr,data):\n",
    "    scales = np.arange(scale_bot,scale_top,step=scale_incr)\n",
    "\n",
    "    cwt = features.plot_wavelet(time, data, scales)\n",
    "    # print(type(cwt))\n",
    "    cwt_features = pd.DataFrame(cwt).T\n",
    "    cwt_features.set_index(returns.index,inplace=True)\n",
    "    return cwt_features\n",
    "\n",
    "def prep_features(data,history_points):\n",
    "    hist = np.array([data[i:i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    return hist\n",
    "\n",
    "def prep_labels(data,history_points):\n",
    "    hist_labels = np.array([data[i + history_points].copy() for i in range(len(data) - history_points)])\n",
    "    hist_labels = np.expand_dims(hist_labels, -1)\n",
    "    return hist_labels\n",
    "\n",
    "def split_data(feats, labels, test_split):\n",
    "    assert feats.shape[0] == labels.shape[0]\n",
    "    n = int(labels.shape[0]*test_split)\n",
    "    feature_train = feats[:n]\n",
    "    label_train = labels[:n]\n",
    "    feature_test = feats[n:]\n",
    "    label_test = labels[n:]\n",
    "    return feature_train, label_train, feature_test, label_test\n",
    "\n",
    "def test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "#     output = Activation('sigmoid', name='linear_output')(x)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "# not used\n",
    "def test2(hist_feats,feature_train,feature_test,label_train,label_test,epoch):\n",
    "    feat_shape_ax1 = hist_feats.shape[1]\n",
    "    feat_shape_ax2 = hist_feats.shape[2]\n",
    "    lstm_input = Input(shape=(feat_shape_ax1, feat_shape_ax2), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(1, name='dense_1')(x)\n",
    "\n",
    "    y = LSTM(50, name='lstm_1')(x)\n",
    "    y = Dropout(0.2, name='lstm_dropout_1')(y)\n",
    "    y = Dense(64, name='dense_0')(y)\n",
    "    y = Activation('sigmoid', name='sigmoid_0')(y)\n",
    "    y = Dense(1, name='dense_1')(y)\n",
    "\n",
    "    output = Activation('sigmoid', name='linear_output')(y)\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    model.fit(x=feature_train, y=label_train, batch_size=batch, epochs=epoch, shuffle=True, validation_split=0.1)\n",
    "    evaluation = model.evaluate(feature_test, label_test)\n",
    "    print(evaluation)\n",
    "\n",
    "    test_predicted = model.predict(feature_test)\n",
    "    # plt.plot(test_predicted,'o')\n",
    "    # plt.plot(label_test,'+')\n",
    "    # plt.legend(['predicted','real'])\n",
    "    # plt.show()\n",
    "    return test_predicted, label_test\n",
    "\n",
    "def test_stats(predicted, real):\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if (predicted[i] > 0) and (real[i] > 0):\n",
    "            c = c+1\n",
    "        if (predicted[i] < 0) and (real[i] < 0):\n",
    "            c = c+1\n",
    "        s = s+1\n",
    "    print('da',c/s)\n",
    "    pct_correct_da = c/s\n",
    "    \n",
    "    return pct_correct_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adjusted close   returns  log-returns  up-down\n",
      "2018-07-02        182.1920       NaN          NaN      NaN\n",
      "2018-07-03        179.0188 -0.017417    -0.017570     -1.0\n",
      "2018-07-05        180.4594  0.008047     0.008015      1.0\n",
      "2018-07-06        182.9609  0.013862     0.013767      1.0\n",
      "2018-07-09        185.5014  0.013885     0.013790      1.0\n",
      "...                    ...       ...          ...      ...\n",
      "2018-12-24        143.9221 -0.025874    -0.026215     -1.0\n",
      "2018-12-26        154.0573  0.070421     0.068052      1.0\n",
      "2018-12-27        153.0575 -0.006490    -0.006511     -1.0\n",
      "2018-12-28        153.1360  0.000513     0.000513      1.0\n",
      "2018-12-31        154.6161  0.009665     0.009619      1.0\n",
      "\n",
      "[126 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "start = '2018-07-01'\n",
    "end = '2019-01-01'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "df = data_reader.download(ticker,start,end)\n",
    "\n",
    "\n",
    "opens = df['adjusted close'].to_frame()\n",
    "opens, returns = calc_returns(opens)\n",
    "print(opens)\n",
    "\n",
    "\n",
    "signal = df['adjusted close'].dropna().to_numpy()\n",
    "log_signal = returns['log-returns'].dropna().to_numpy()\n",
    "\n",
    "\n",
    "data = log_signal\n",
    "N = len(data)\n",
    "t0=0\n",
    "dt=1/365\n",
    "time = np.arange(0, N) * dt + t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = returns['log-returns'].dropna().to_numpy() \n",
    "scale_bot = 1\n",
    "scale_top = 10\n",
    "scale_incr = 1\n",
    "cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num lbls: 125\n",
      "num results: 126\n"
     ]
    }
   ],
   "source": [
    "results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "print('num lbls:',len(lbls))\n",
    "print('num results:',len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adjusted close   returns  log-returns  up-down\n",
      "2002-01-02          1.4407       NaN          NaN      NaN\n",
      "2002-01-03          1.4580  0.012008     0.011937      1.0\n",
      "2002-01-04          1.4648  0.004664     0.004653      1.0\n",
      "2002-01-07          1.4159 -0.033383    -0.033953     -1.0\n",
      "2002-01-08          1.3980 -0.012642    -0.012723     -1.0\n",
      "...                    ...       ...          ...      ...\n",
      "2019-01-04        145.3238  0.042689     0.041803      1.0\n",
      "2019-01-07        145.0003 -0.002226    -0.002229     -1.0\n",
      "2019-01-08        147.7645  0.019063     0.018884      1.0\n",
      "2019-01-09        150.2738  0.016982     0.016839      1.0\n",
      "2019-01-10        150.7541  0.003196     0.003191      1.0\n",
      "\n",
      "[4286 rows x 4 columns]\n",
      "running: hist 10, cwt top 10,batch 8\n",
      "WARNING:tensorflow:From c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py:853: RuntimeWarning: invalid value encountered in sign\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\danie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3462 samples, validate on 385 samples\n",
      "Epoch 1/150\n",
      "3462/3462 [==============================] - 8s 2ms/step - loss: 0.0012 - val_loss: 2.1675e-04\n",
      "Epoch 2/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.7173e-04 - val_loss: 2.2690e-04\n",
      "Epoch 3/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 5.8562e-04 - val_loss: 3.7642e-04\n",
      "Epoch 4/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.8751e-04 - val_loss: 2.1011e-04\n",
      "Epoch 5/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.7174e-04 - val_loss: 2.2745e-04\n",
      "Epoch 6/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.9738e-04 - val_loss: 2.1519e-04\n",
      "Epoch 7/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.8522e-04 - val_loss: 3.7244e-04\n",
      "Epoch 8/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.7198e-04 - val_loss: 1.9993e-04\n",
      "Epoch 9/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.9587e-04 - val_loss: 2.5992e-04\n",
      "Epoch 10/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.8366e-04 - val_loss: 2.4667e-04\n",
      "Epoch 11/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.7628e-04 - val_loss: 1.9428e-04\n",
      "Epoch 12/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.6412e-04 - val_loss: 2.0381e-04\n",
      "Epoch 13/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.6135e-04 - val_loss: 2.0907e-04\n",
      "Epoch 14/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.7943e-04 - val_loss: 2.0487e-04\n",
      "Epoch 15/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.6135e-04 - val_loss: 3.8865e-04\n",
      "Epoch 16/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.6885e-04 - val_loss: 2.2651e-04\n",
      "Epoch 17/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.4933e-04 - val_loss: 2.1605e-04\n",
      "Epoch 18/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.5346e-04 - val_loss: 2.3668e-04\n",
      "Epoch 19/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.5056e-04 - val_loss: 3.9520e-04\n",
      "Epoch 20/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.3971e-04 - val_loss: 3.1895e-04\n",
      "Epoch 21/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.3496e-04 - val_loss: 2.0913e-04\n",
      "Epoch 22/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.3291e-04 - val_loss: 2.1506e-04\n",
      "Epoch 23/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.1297e-04 - val_loss: 3.3080e-04\n",
      "Epoch 24/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.2310e-04 - val_loss: 3.3166e-04\n",
      "Epoch 25/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.1141e-04 - val_loss: 2.1803e-04\n",
      "Epoch 26/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 5.1719e-04 - val_loss: 1.8595e-04\n",
      "Epoch 27/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.8849e-04 - val_loss: 2.0055e-04\n",
      "Epoch 28/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.7186e-04 - val_loss: 1.8946e-04\n",
      "Epoch 29/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.7916e-04 - val_loss: 1.9190e-04\n",
      "Epoch 30/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.5075e-04 - val_loss: 1.8522e-04\n",
      "Epoch 31/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.4426e-04 - val_loss: 2.4234e-04\n",
      "Epoch 32/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.5607e-04 - val_loss: 1.8650e-04\n",
      "Epoch 33/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 4.6083e-04 - val_loss: 1.8019e-04\n",
      "Epoch 34/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.5072e-04 - val_loss: 1.8014e-04\n",
      "Epoch 35/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.3572e-04 - val_loss: 2.1633e-04\n",
      "Epoch 36/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.2790e-04 - val_loss: 1.7716e-04\n",
      "Epoch 37/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 4.2820e-04 - val_loss: 2.4016e-04\n",
      "Epoch 38/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.2143e-04 - val_loss: 1.6402e-04\n",
      "Epoch 39/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.0591e-04 - val_loss: 1.7042e-04\n",
      "Epoch 40/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.1768e-04 - val_loss: 1.8127e-04\n",
      "Epoch 41/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 4.1250e-04 - val_loss: 1.7309e-04\n",
      "Epoch 42/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.8590e-04 - val_loss: 1.9451e-04\n",
      "Epoch 43/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.7892e-04 - val_loss: 2.1654e-04\n",
      "Epoch 44/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.7931e-04 - val_loss: 1.5832e-04\n",
      "Epoch 45/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.5905e-04 - val_loss: 1.6935e-04\n",
      "Epoch 46/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.7727e-04 - val_loss: 1.7052e-04\n",
      "Epoch 47/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.6213e-04 - val_loss: 2.5806e-04\n",
      "Epoch 48/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.4604e-04 - val_loss: 2.1538e-04\n",
      "Epoch 49/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.4728e-04 - val_loss: 1.6686e-04\n",
      "Epoch 50/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 3.4935e-04 - val_loss: 2.5593e-04\n",
      "Epoch 51/150\n",
      "3462/3462 [==============================] - 5s 2ms/step - loss: 3.3144e-04 - val_loss: 2.3965e-04\n",
      "Epoch 52/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.2240e-04 - val_loss: 1.5847e-04\n",
      "Epoch 53/150\n",
      "3462/3462 [==============================] - 5s 2ms/step - loss: 3.5180e-04 - val_loss: 1.6056e-04\n",
      "Epoch 54/150\n",
      "3462/3462 [==============================] - 5s 2ms/step - loss: 3.3392e-04 - val_loss: 2.2755e-04\n",
      "Epoch 55/150\n",
      "3462/3462 [==============================] - 5s 2ms/step - loss: 3.0791e-04 - val_loss: 1.5204e-04\n",
      "Epoch 56/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.2012e-04 - val_loss: 1.8628e-04\n",
      "Epoch 57/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.1212e-04 - val_loss: 2.1465e-04\n",
      "Epoch 58/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 3.2956e-04 - val_loss: 1.6178e-04\n",
      "Epoch 59/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.1693e-04 - val_loss: 1.6121e-04\n",
      "Epoch 60/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.8881e-04 - val_loss: 2.1336e-04\n",
      "Epoch 61/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.1279e-04 - val_loss: 2.1058e-04\n",
      "Epoch 62/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.0641e-04 - val_loss: 1.7434e-04\n",
      "Epoch 63/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.9285e-04 - val_loss: 3.4197e-04\n",
      "Epoch 64/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 3.0496e-04 - val_loss: 2.8853e-04\n",
      "Epoch 65/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.7892e-04 - val_loss: 1.5380e-04\n",
      "Epoch 66/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.7319e-04 - val_loss: 4.1521e-04\n",
      "Epoch 67/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.8032e-04 - val_loss: 1.6409e-04\n",
      "Epoch 68/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 2.7111e-04 - val_loss: 2.3995e-04\n",
      "Epoch 69/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 2.8334e-04 - val_loss: 1.4782e-04\n",
      "Epoch 70/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.6982e-04 - val_loss: 1.6875e-04\n",
      "Epoch 71/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.7067e-04 - val_loss: 1.5757e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 2.8479e-04 - val_loss: 1.5547e-04\n",
      "Epoch 73/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.6636e-04 - val_loss: 1.6830e-04\n",
      "Epoch 74/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.5285e-04 - val_loss: 1.8402e-04\n",
      "Epoch 75/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 2.7787e-04 - val_loss: 3.1703e-04\n",
      "Epoch 76/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.8266e-04 - val_loss: 2.4300e-04\n",
      "Epoch 77/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.5347e-04 - val_loss: 1.5158e-04\n",
      "Epoch 78/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.4737e-04 - val_loss: 1.4734e-04\n",
      "Epoch 79/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3951e-04 - val_loss: 2.0976e-04\n",
      "Epoch 80/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.5706e-04 - val_loss: 2.0251e-04\n",
      "Epoch 81/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.4568e-04 - val_loss: 1.4482e-04\n",
      "Epoch 82/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.4912e-04 - val_loss: 1.4613e-04\n",
      "Epoch 83/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3989e-04 - val_loss: 1.4454e-04\n",
      "Epoch 84/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3994e-04 - val_loss: 1.9192e-04\n",
      "Epoch 85/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3698e-04 - val_loss: 1.5151e-04\n",
      "Epoch 86/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3664e-04 - val_loss: 1.3181e-04\n",
      "Epoch 87/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3490e-04 - val_loss: 1.4231e-04\n",
      "Epoch 88/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3411e-04 - val_loss: 1.8370e-04\n",
      "Epoch 89/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1663e-04 - val_loss: 1.3688e-04\n",
      "Epoch 90/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3920e-04 - val_loss: 2.0947e-04\n",
      "Epoch 91/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.2533e-04 - val_loss: 2.9993e-04\n",
      "Epoch 92/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.4124e-04 - val_loss: 2.0796e-04\n",
      "Epoch 93/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.2074e-04 - val_loss: 1.3789e-04\n",
      "Epoch 94/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3036e-04 - val_loss: 1.3831e-04\n",
      "Epoch 95/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1847e-04 - val_loss: 1.5877e-04\n",
      "Epoch 96/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1826e-04 - val_loss: 1.3839e-04\n",
      "Epoch 97/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1265e-04 - val_loss: 1.4577e-04\n",
      "Epoch 98/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1596e-04 - val_loss: 1.3884e-04\n",
      "Epoch 99/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.3386e-04 - val_loss: 1.3203e-04\n",
      "Epoch 100/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 2.1131e-04 - val_loss: 1.2902e-04\n",
      "Epoch 101/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0549e-04 - val_loss: 1.3165e-04\n",
      "Epoch 102/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.2747e-04 - val_loss: 2.0011e-04\n",
      "Epoch 103/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.2082e-04 - val_loss: 1.4043e-04\n",
      "Epoch 104/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0186e-04 - val_loss: 5.5457e-04\n",
      "Epoch 105/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1098e-04 - val_loss: 1.4947e-04\n",
      "Epoch 106/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0119e-04 - val_loss: 1.3381e-04\n",
      "Epoch 107/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.9788e-04 - val_loss: 1.3025e-04\n",
      "Epoch 108/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0161e-04 - val_loss: 1.4003e-04\n",
      "Epoch 109/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.1574e-04 - val_loss: 1.3381e-04\n",
      "Epoch 110/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0168e-04 - val_loss: 1.6083e-04\n",
      "Epoch 111/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0860e-04 - val_loss: 1.9866e-04\n",
      "Epoch 112/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8425e-04 - val_loss: 2.2718e-04\n",
      "Epoch 113/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.9237e-04 - val_loss: 1.8923e-04\n",
      "Epoch 114/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.9757e-04 - val_loss: 1.2424e-04\n",
      "Epoch 115/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 2.0127e-04 - val_loss: 1.2295e-04\n",
      "Epoch 116/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.9499e-04 - val_loss: 1.2205e-04\n",
      "Epoch 117/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.9455e-04 - val_loss: 2.9048e-04\n",
      "Epoch 118/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8736e-04 - val_loss: 1.2403e-04\n",
      "Epoch 119/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8825e-04 - val_loss: 1.5200e-04\n",
      "Epoch 120/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8701e-04 - val_loss: 1.4150e-04\n",
      "Epoch 121/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8714e-04 - val_loss: 1.4177e-04\n",
      "Epoch 122/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8016e-04 - val_loss: 1.1621e-04\n",
      "Epoch 123/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7603e-04 - val_loss: 1.4601e-04\n",
      "Epoch 124/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.9151e-04 - val_loss: 1.2172e-04\n",
      "Epoch 125/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7889e-04 - val_loss: 1.1656e-04\n",
      "Epoch 126/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7551e-04 - val_loss: 1.4513e-04\n",
      "Epoch 127/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.8287e-04 - val_loss: 1.3403e-04\n",
      "Epoch 128/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7048e-04 - val_loss: 1.2587e-04\n",
      "Epoch 129/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.6489e-04 - val_loss: 1.2111e-04\n",
      "Epoch 130/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.6344e-04 - val_loss: 1.2735e-04\n",
      "Epoch 131/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.6923e-04 - val_loss: 1.1941e-04\n",
      "Epoch 132/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.6966e-04 - val_loss: 1.1616e-04\n",
      "Epoch 133/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7082e-04 - val_loss: 1.5169e-04\n",
      "Epoch 134/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.7587e-04 - val_loss: 1.5185e-04\n",
      "Epoch 135/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.4872e-04 - val_loss: 1.3782e-04\n",
      "Epoch 136/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.6076e-04 - val_loss: 2.1891e-04\n",
      "Epoch 137/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.6895e-04 - val_loss: 1.2330e-04\n",
      "Epoch 138/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.6524e-04 - val_loss: 1.5640e-04\n",
      "Epoch 139/150\n",
      "3462/3462 [==============================] - 7s 2ms/step - loss: 1.6290e-04 - val_loss: 1.2766e-04\n",
      "Epoch 140/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5024e-04 - val_loss: 1.1306e-04\n",
      "Epoch 141/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.6221e-04 - val_loss: 1.3773e-04\n",
      "Epoch 142/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5738e-04 - val_loss: 1.5217e-04\n",
      "Epoch 143/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5331e-04 - val_loss: 1.3318e-04\n",
      "Epoch 144/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5683e-04 - val_loss: 1.0915e-04\n",
      "Epoch 145/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5566e-04 - val_loss: 1.7014e-04\n",
      "Epoch 146/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.5236e-04 - val_loss: 1.8440e-04\n",
      "Epoch 147/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.4751e-04 - val_loss: 1.0901e-04\n",
      "Epoch 148/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.4668e-04 - val_loss: 1.9781e-04\n",
      "Epoch 149/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.4996e-04 - val_loss: 1.3048e-04\n",
      "Epoch 150/150\n",
      "3462/3462 [==============================] - 6s 2ms/step - loss: 1.4659e-04 - val_loss: 1.1737e-04\n",
      "428/428 [==============================] - 0s 221us/step\n",
      "0.00011031695306475196\n",
      "da 0.7850467289719626\n"
     ]
    }
   ],
   "source": [
    "start = '2002-01-01'\n",
    "end = '2019-01-10'\n",
    "ticker = 'AAPL'\n",
    "\n",
    "df = data_reader.download(ticker,start,end)\n",
    "\n",
    "\n",
    "opens = df['adjusted close'].to_frame()\n",
    "opens, returns = calc_returns(opens)\n",
    "print(opens)\n",
    "\n",
    "\n",
    "signal = df['adjusted close'].dropna().to_numpy()\n",
    "log_signal = returns['log-returns'].dropna().to_numpy()\n",
    "\n",
    "\n",
    "data = log_signal\n",
    "N = len(data)\n",
    "t0=0\n",
    "dt=1/365\n",
    "time = np.arange(0, N) * dt + t0\n",
    "\n",
    "\n",
    "# scale_bot = 1 ########## UNCOMMENT ##########\n",
    "# scale_top = 80 ########## UNCOMMENT ##########\n",
    "# scale_incr = 1 ########## UNCOMMENT ##########\n",
    "\n",
    "# cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data) ########## UNCOMMENT ##########\n",
    "# results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False) ########## UNCOMMENT ##########\n",
    "# results = pd.concat([opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "# print(results)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# feats = cwt_features.to_numpy()\n",
    "# feats = results.dropna().to_numpy() ########## UNCOMMENT ##########\n",
    "###################################################################\n",
    "\n",
    "# history_points = 100\n",
    "\n",
    "# hist_cwt = prep_features(cwt_features_np,history_points) ####### either this or next line. try next\n",
    "# hist_feats = prep_features(feats,history_points) ########## UNCOMMENT ##########\n",
    "\n",
    "\n",
    "lbls = returns['log-returns'].dropna().to_numpy() \n",
    "# hist_labels = prep_labels(lbls,history_points) ########## UNCOMMENT ##########\n",
    "# print(results['log-returns'])\n",
    "# print(hist_feats)\n",
    "# print(hist_labels)\n",
    "\n",
    "# print(hist_labels.shape)\n",
    "# print(hist_feats.shape)\n",
    "\n",
    "test_split = 0.9\n",
    "\n",
    "# feature_train, label_train, feature_test, label_test = split_data(hist_feats,hist_labels,0.9) ########## UNCOMMENT ##########\n",
    "\n",
    "# print('feat train',feature_train.shape)\n",
    "# print('label train',label_train.shape)\n",
    "# print('feat test',feature_test.shape)\n",
    "# print('label test',label_test.shape)\n",
    "\n",
    "\n",
    "# epoch = 100 ########## UNCOMMENT ##########\n",
    "# batch = 32 ########## UNCOMMENT ##########\n",
    "# predicted, real = test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch) ########## UNCOMMENT ##########\n",
    "# pct_da = test_stats(predicted, real) ########## UNCOMMENT ##########\n",
    "# print('epoch:',epoch) ########## UNCOMMENT ##########\n",
    "# print('startdate:',start) ########## UNCOMMENT ##########\n",
    "# print('enddate:',end) ########## UNCOMMENT ##########\n",
    "# print('cwt splits:',scale_bot,scale_top,scale_incr) ########## UNCOMMENT ##########\n",
    "# print('hist dates',history_points) ########## UNCOMMENT ##########\n",
    "\n",
    "report = pd.DataFrame(columns=['Epoch','Batch Size','CWT Top','CWT Incr','Hist points','Pct Acc'])\n",
    "\n",
    "\n",
    "batches = [8]\n",
    "tops = [10]\n",
    "hist_list = [10]\n",
    "epochs = [150]\n",
    "for i in tops:\n",
    "    for b in batches:\n",
    "        for k in hist_list:\n",
    "            for e in epochs:\n",
    "                \n",
    "                print(\"running: hist \" + str(k) + \", cwt top \"+ str(i) + \",batch \"+str(b) )\n",
    "                scale_bot = 1\n",
    "                scale_top = i\n",
    "                scale_incr = 1\n",
    "\n",
    "                cwt_features = get_cwt_features(scale_bot,scale_top,scale_incr,data)\n",
    "                results = pd.concat([opens['up-down'],opens['log-returns'],cwt_features],axis=1,sort=False)\n",
    "\n",
    "                feats = results.dropna().to_numpy()\n",
    "                history_points = k\n",
    "                hist_feats = prep_features(feats,history_points)\n",
    "\n",
    "                # lbls = returns['log-returns'].dropna().to_numpy()\n",
    "                hist_labels = prep_labels(lbls,history_points)\n",
    "\n",
    "                \n",
    "                feature_train, label_train, feature_test, label_test = split_data(hist_feats,hist_labels,0.9)\n",
    "\n",
    "                epoch = e\n",
    "                batch = b \n",
    "                predicted, real = test(hist_feats,feature_train,feature_test,label_train,label_test,epoch,batch) \n",
    "                pct_da = test_stats(predicted, real) \n",
    "\n",
    "                report = report.append({'Epoch':e,'Batch Size':b,'CWT Top':i,'CWT Incr':1,'Hist points':k,'Pct Acc':pct_da},ignore_index=True)\n",
    "                \n",
    "                test_datapoints = pd.DataFrame(data={'Pred':predicted.T[0],'Actual':real.T[0]})\n",
    "                fname_test_dpoints = str(ticker)+'_datapoints_e'+str(e)+'b'+str(b)+'cwttop'+str(i)+'cwtinc'+str(1)+'h'+str(k)+'pctacc'+str(pct_da)+'.csv'\n",
    "                data_reader.save_df(test_datapoints,fname_test_dpoints)\n",
    "\n",
    "data_reader.save_df(report,'AAPL_hypertests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
